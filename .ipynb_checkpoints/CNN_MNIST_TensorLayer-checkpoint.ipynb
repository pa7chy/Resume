{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorlayer as tl\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "from tensorlayer.layers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[TL] Load or Download MNIST > data\\mnist\n",
      "[TL] data\\mnist\\train-images-idx3-ubyte.gz\n",
      "[TL] data\\mnist\\t10k-images-idx3-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_val, y_val, x_test, y_test = tl.files.load_mnist_dataset(shape=(-1, 28, 28, 1))\n",
    "config = tf.ConfigProto(\n",
    "#     allow_soft_placement=True,\n",
    "#     log_device_placement=True\n",
    "#     device_count = {'GPU':0}\n",
    ")\n",
    "# config.gpu_options.per_process_gpu_memory_fraction = 0.5    \n",
    "# config.gpu_options.allow_growth = True      \n",
    "# config.gpu_options.allocator_type = 'BFC'  \n",
    "sess = tf.InteractiveSession(config=config)\n",
    "batch_size = 50\n",
    "x = tf.placeholder(tf.float32, shape=[batch_size, 28, 28, 1])\n",
    "y_ = tf.placeholder(tf.int64, shape=[batch_size,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[TL] InputLayer  input: (50, 28, 28, 1)\n",
      "[TL] Conv2dLayer cnn1: shape:(5, 5, 1, 32) strides:(1, 1, 1, 1) pad:SAME act:relu\n",
      "[TL] PoolLayer   pool1: ksize:[1, 2, 2, 1] strides:[1, 2, 2, 1] padding:SAME pool:max_pool\n"
     ]
    }
   ],
   "source": [
    "network = InputLayer(x, name='input')\n",
    "network = Conv2d(network, n_filter=32, filter_size=(5,5), strides=(1, 1), act=tf.nn.relu, padding='SAME', name='cnn1')\n",
    "network = MaxPool2d(network, filter_size=(2,2), strides=(2, 2), padding='SAME', name='pool1')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "##Tensorflow\n",
    "h_conv1 = tf.nn.relu(tf.nn.conv2d(x_image, tf.truncated_normal([5, 5, 1, 32],stddev=0.1),strides=[1, 1, 1, 1], padding='SAME') + tf.constant(0.1, shape=[32]))\n",
    "h_pool1 = tf.nn.max_pool(h_conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[TL] Conv2dLayer cnn2: shape:(5, 5, 32, 64) strides:(1, 1, 1, 1) pad:SAME act:relu\n",
      "[TL] PoolLayer   pool2: ksize:[1, 2, 2, 1] strides:[1, 2, 2, 1] padding:SAME pool:max_pool\n"
     ]
    }
   ],
   "source": [
    "network = Conv2d(network, n_filter=64, filter_size=(5,5), strides=(1,1), act=tf.nn.relu, padding='SAME', name='cnn2')\n",
    "network = MaxPool2d(network, filter_size=(2,2), strides=(2,2), padding='SAME', name='pool2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[TL] FlattenLayer flatten: 3136\n"
     ]
    }
   ],
   "source": [
    "network = FlattenLayer(network, name='flatten')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "##Tensorflow\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[TL] DropoutLayer drop1: keep:0.500000 is_fix:False\n",
      "[TL] DenseLayer  fc1: 256 relu\n",
      "[TL] DropoutLayer drop2: keep:0.500000 is_fix:False\n",
      "[TL] DenseLayer  fc2_output10: 10 identity\n"
     ]
    }
   ],
   "source": [
    "network = DropoutLayer(network, keep=0.5, name='drop1')\n",
    "network = DenseLayer(network, 256, tf.nn.relu, name='fc1')\n",
    "network = DropoutLayer(network, keep=0.5, name='drop2')\n",
    "network = DenseLayer(network, 10, tf.identity, name='fc2_output10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_conv = network.outputs\n",
    "cross_entropy = tl.cost.cross_entropy(y_conv, y_, 'cross_entropy')\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1),y_)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Define model\n",
    "y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "\n",
    "#Loss measurement\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_conv,labels=y_))\n",
    "\n",
    "#correct\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "#accurate\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[TL]   param   0: cnn1/W_conv2d:0      (5, 5, 1, 32)      float32_ref (mean: 0.0004674112133216113, median: 0.001299771131016314, std: 0.01770349033176899)   \n",
      "[TL]   param   1: cnn1/b_conv2d:0      (32,)              float32_ref (mean: 0.0               , median: 0.0               , std: 0.0               )   \n",
      "[TL]   param   2: cnn2/W_conv2d:0      (5, 5, 32, 64)     float32_ref (mean: 2.6017287382273935e-05, median: 8.26036739454139e-06, std: 0.01761384680867195)   \n",
      "[TL]   param   3: cnn2/b_conv2d:0      (64,)              float32_ref (mean: 0.0               , median: 0.0               , std: 0.0               )   \n",
      "[TL]   param   4: fc1/W:0              (3136, 256)        float32_ref (mean: -9.184061491396278e-05, median: -0.00018758147780317813, std: 0.08802571147680283)   \n",
      "[TL]   param   5: fc1/b:0              (256,)             float32_ref (mean: 0.0               , median: 0.0               , std: 0.0               )   \n",
      "[TL]   param   6: fc2_output10/W:0     (256, 10)          float32_ref (mean: -0.0008214473491534591, median: -0.003702712245285511, std: 0.08762798458337784)   \n",
      "[TL]   param   7: fc2_output10/b:0     (10,)              float32_ref (mean: 0.0               , median: 0.0               , std: 0.0               )   \n",
      "[TL]   num of params: 857738\n",
      "[TL]   layer   0: cnn1/Relu:0          (50, 28, 28, 32)    float32\n",
      "[TL]   layer   1: pool1:0              (50, 14, 14, 32)    float32\n",
      "[TL]   layer   2: cnn2/Relu:0          (50, 14, 14, 64)    float32\n",
      "[TL]   layer   3: pool2:0              (50, 7, 7, 64)     float32\n",
      "[TL]   layer   4: flatten:0            (50, 3136)         float32\n",
      "[TL]   layer   5: drop1/mul:0          (50, 3136)         float32\n",
      "[TL]   layer   6: fc1/Relu:0           (50, 256)          float32\n",
      "[TL]   layer   7: drop2/mul:0          (50, 256)          float32\n",
      "[TL]   layer   8: fc2_output10/Identity:0 (50, 10)           float32\n"
     ]
    }
   ],
   "source": [
    "train_params = network.all_params\n",
    "train_op = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy, var_list=train_params)\n",
    "\n",
    "tl.layers.initialize_global_variables(sess)\n",
    "network.print_params()\n",
    "network.print_layers()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Loss optimization\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "n_epoch = 20\n",
    "print_freq = 2\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "    for x_train_a, y_train_a in tl.iterate.minibatches(x_train, y_train, batch_size, shuffle=True):\n",
    "        feed_dict = {x: x_train_a, y_: y_train_a}\n",
    "        feed_dict.update(network.all_drop)\n",
    "        sess.run(train_op, feed_dict=feed_dict)\n",
    "    if epoch+1 == 1 or (epoch+1) % print_freq == 0:\n",
    "        print(\"Epoch %d of %d took %fs\" % (epoch+1, n_epoch, time.time()-start_time))\n",
    "        train_loss, train_acc, n_batch = 0, 0, 0\n",
    "        for x_train_a, y_train_a in tl.iterate.minibatches(x_train, y_train, batch_size, shuffle=True):\n",
    "            dp_dict = tl.utils.dict_to_one(network.all_drop)\n",
    "            feed_dict = {x: x_train_a, y_: y_train_a}\n",
    "            feed_dict.update(dp_dict)\n",
    "            err, ac = sess.run([cross_entropy, accuracy], feed_dict=feed_dict)\n",
    "            train_loss += err; train_acc += ac; n_batch += 1\n",
    "        print(\"  train loss: %f\" % (train_loss/n_batch))\n",
    "        print(\"  train acc: %f\" % (train_acc/n_batch))\n",
    "            \n",
    "        val_loss, val_acc, n_batch = 0, 0, 0\n",
    "        for x_val_a, y_val_a in tl.iterate.minibatches(x_val, y_val, batch_size, shuffle=True):\n",
    "            dp_dict = tl.utils.dict_to_one(network.all_drop)\n",
    "            feed_dict = {x: x_train_a, y_: y_train_a}\n",
    "            feed_dict.update(dp_dict)\n",
    "            err, ac = sess.run([cross_entropy, accuracy], feed_dict=feed_dict)\n",
    "            val_loss += err; val_acc += ac; n_batch += 1\n",
    "        print(\"  val loss: %f\" % (val_loss/n_batch))\n",
    "        print(\"  val acc: %f\" % (val_acc/n_batch))\n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "num_steps = 2000\n",
    "display_every = 100\n",
    "\n",
    "#start timer\n",
    "start_time = time.time()\n",
    "end_time = time.time()\n",
    "\n",
    "for i in range(num_steps):\n",
    "    batch = mnist.train.next_batch(50)\n",
    "    train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob:0.5})\n",
    "    \n",
    "    if i%display_every == 0:\n",
    "        train_accuracy = accuracy.eval(feed_dict={x:batch[0], y_:batch[1], keep_prob: 1.0})\n",
    "        end_time = time.time()\n",
    "        print(\"step {0}, elapsed time {1:.2f} seconds, training accuracy {2:.3f}%\".format(i, end_time - start_time, train_accuracy*100))\n",
    "#Display summary\n",
    "end_time = time.time()\n",
    "batch_size = 50\n",
    "batch_num = int(mnist.test.num_examples / batch_size)\n",
    "test_accuracy = 0\n",
    "    \n",
    "for i in range(batch_num):\n",
    "    batch = mnist.test.next_batch(batch_size)\n",
    "    test_accuracy += accuracy.eval(feed_dict={x: batch[0],\n",
    "                                              y_: batch[1],\n",
    "                                              keep_prob: 1.0})\n",
    "\n",
    "test_accuracy /= batch_num\n",
    "print(\"Test accuracy {0:.3f}%\".format(test_accuracy*100.0))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[TL] Start training the network ...\n",
      "[TL] Epoch 1 of 10 took 6.572580s\n",
      "[TL]    val loss: 0.152223\n",
      "[TL]    val acc: 0.957100\n",
      "[TL] Epoch 5 of 10 took 6.006128s\n",
      "[TL]    val loss: 0.057862\n",
      "[TL]    val acc: 0.983000\n",
      "[TL] Epoch 10 of 10 took 6.099233s\n",
      "[TL]    val loss: 0.039203\n",
      "[TL]    val acc: 0.988400\n",
      "[TL] Total training time: 62.155718s\n",
      "[TL] Start testing the network ...\n",
      "[TL]    test loss: 0.030749\n",
      "[TL]    test acc: 0.989400\n"
     ]
    }
   ],
   "source": [
    "tl.utils.fit(sess, network, train_op, cross_entropy, x_train, y_train, x, y_, acc=accuracy, batch_size=batch_size, n_epoch=10, print_freq=5, X_val=x_val, y_val=y_val, eval_train=False)\n",
    "tl.utils.test(sess, network, accuracy, x_test, y_test, x, y_, batch_size=batch_size, cost=cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### In line compared Tensor Layer with Tensor Flow. Tensor Layer cost much fewer lines for same function. But drops performance when the mini batch size is small with unknown reason."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
